---
title: "Data Manipulation"
output: html_document
---

We need to load some necessary packages first. We'll just use the `tidyverse` package for doing our data manipulation and analysis.

```{r, include=FALSE}
library(tidyverse)
```

And let's read some data that we'll be working with.

```{r}
coffee_data <- read_csv("https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv")

# Remove the blank column and clean up dates
coffee_data$X1 <- NULL

# We separate and tidy the harvest date for some time series analysis
library(lubridate)
coffee_data$date <- mdy(coffee_data$Grading.Date)
coffee_data$year <- year(coffee_data$date)

# Quick look of the data
head(coffee_data)
```

The data here contains reviews of 1,312 arabica coffee beans from the Coffee Quality Institute's trained reviewers (thanks to [jldbc](https://github.com/jldbc/coffee-quality-database). In general, the data tells us:

Quality Measures

- Aroma
- Flavor
- Aftertaste
- Acidity
- Body
- Balance
- Uniformity
- Cup Cleanliness
- Sweetness
- Moisture
- Defects

Bean Metadata

- Processing Method
- Color
- Species

Farm Metadata

- Owner
- Country of Origin
- Farm Name
- Lot Number
- Mill
- Company
- Altitude
- Region

And we've added two new columns:

- `date` -- the information from the `Grade.Date` converted into a format R recognizes.
- `year` -- just the year from the `Grade.Date` column.

# Select

We can select which columns we want by putting the name of the column in the `select()` function. Here we pick three columns.

```{r}
coffee_data %>% 
  select(Species, Owner, Country.of.Origin)
```

We can also get rid of columns by using the `-` sign. The `starts_with()` and `ends_with()` functions are useful.

Read the documentation for this function, `?select`.

Select the `Farm.Name` and `Company` columns, and all the columns that start with the word `Category`.

```{r}

```

Remove the column `Species`.

```{r}

```

Pick just the columns that you want:

```{r}

```

# Filter

Filtering is more interesting. To keep certain rows, we have to pass the `filter()` function a vector of `TRUE` and `FALSE` values, one for each row. The most common way to do that is to use a comparison operator on a column of the data.

```{r}
coffee_data %>% 
  filter(Country.of.Origin == "Ethiopia")
```

Or we can look at the `Flavor` grades that are above 8. 

```{r}
coffee_data %>% 
  filter(Flavor > 8) 
```

Can you get just the farms with cupping points more than 7?

```{r}

```

Can you get just the farms with aroma between 6 and 8?

```{r}

```

Get just the rows from Guatemala with cupping points above 8.4.

```{r}

```

# Arrange

The `arrange()` function lets us sort. Often we want to sort a data frame by one of its columns. This can be done with the verb `arrange()`. By default `arrange()` will sort from least to greatest; we can use the function `desc()` to sort from greatest to least. In this example, we sort the data frame to get the counties with the highest number of people. Here are the biggest counties in 1830. (Notice that we can create a pipeline of functions.)

```{r}
coffee_data %>% 
  select(Country.of.Origin, Owner, Total.Cup.Points) %>% 
  filter(Country.of.Origin == "United States") %>% 
  arrange(desc(Total.Cup.Points))
```

Which countries produce the greatest number of coffee bags?

```{r}

```

# Mutate

The `mutate()` function lets us create new columns out of existing columns. Perhaps you would like a column that calculates the total cup points each farm.

```{r}
coffee_data %>%
  mutate(total_cup_points = Aroma + Flavor + Aftertaste + Acidity + Body + Balance + Uniformity + Clean.Cup + Sweetness + Cupper.Points)
```

Can you create a column called `flavor_score` out of `Aroma`, `Flavor`, and `Acidity`?

```{r}

```

# Group by and summarize

Notice that in the example above the `arrange()` function sorted the entire data frame. So when we looked for the largest countries of origin, we got rows from one country, then another, then another, and so on. What if we wanted to get the biggest country for each farm?

We can solve this kind of problem with what Hadley Wickham calls the "split-apply-combine" pattern of data analysis. Think of it this way. First we can *split* the big data frame into separate data frames, one for each farm. Then we can *apply* our logic to get the results we want; in this case, that means sorting the data frame. We might also want to get just the top one row with the largest number of beans produced. Then we can *combine* those split apart data frames into a new data frame.

Observe how this works. If we want to get the country of origin with the highest number of bean producers in each farm, we can use the following code:

```{r}
coffee_data %>% 
  select(Owner, Country.of.Origin, Number.of.Bags) %>% 
  group_by(Country.of.Origin) %>% 
  arrange(desc(Number.of.Bags)) %>% 
  slice(1) 
```

Let's walk through that logic step by step. 

1. First, we select only the columns that we are interested in, namely, the column for the farm owner, the country of origin, and the number of bags of coffee produced. We do this just so that the results print out in a useful way: in a real analysis we might decide not to throw away the other columns.
2. The crucial step is when we `group_by()` the `Country.of.Origin`. This creates a new data-frame (the *split* step) for each unique combination of values in the variables. (Note that you can group by combinations of columns, so, one group for each combination of farm and owner, for instance.)
4. Next we *apply* our logic, in this case, sorting by the column `Number.of.Bags` in descending order. This puts the rows with the biggest value at the top.
5. Next we continue to *apply* our logic with `slice()`. This function simply gives us the rows in each of the split-up data frames with that index. So `slice(1)` gives us the first row, `slice(5)` gives us this fifth row, and `slice(1:5)` gives us the first through fifth rows. 
6. The last step, *combine*, where the split-up data frames are brought back together, is done for us automatically. Note that the data frame is still grouped, however, so any subsequent data manipulation verbs will be applied to the groups rather than the whole data frame. If we wished, we could use `ungroup()`.

This particular operation, getting the top value in a split up data frame is so common that dplyr provides us with a `top_n()` function as a short cut. That function also handles ties better. (What if, for instance, two countries both have the same biggest value?)

```{r}
coffee_data %>% 
  select(Owner, Number.of.Bags, Country.of.Origin) %>% 
  group_by(Country.of.Origin) %>% 
  top_n(1, Number.of.Bags)
```

We get the same results more concisely and reliably, though the steps of "split-apply-combine" are perhaps somewhat less easy to see.

For each country, which was the biggest producer?

```{r}

```

For the year 2012, what was the largest country of origin?

```{r}

```

For each year, what were the three largest producers by country of origin?

```{r}

```

## Summarizing or aggregating data (`summarize()`)

In the examples using `top_n()` or `slice()` we performed a very simple kind of data summary, where we took the single row with the biggest value in a given column. This essentially boiled many rows of a data frame down into a single row. We would like to be able to summarize or aggregate a data frame in other ways as well. For instance, we often want to take the sum or the mean of a given column. We can do this using the `summarize()` function in conjunction with the `group_by()` function.

In this example, we group by the country of origin. Then we find the total number of farms for each country.

```{r}
coffee_data %>% 
  group_by(Country.of.Origin) %>% 
  summarise(total_farms = sum(Number.of.Bags, na.rm = TRUE))
```

Notice that we get one row in the recombined data frame for each group in the original data frame. The value in the new column is the result of a function (in this case, `sum()`) applied to the columns in each of the split apart data frames.

There is also a special case where we might want to know how many rows were in each of the split apart (or grouped) data frames. We can use the special `n()` function to get that count. (Just like the case of `slice()` and `top_n()`, this is such a common thing to do that dplyr provides the special functions `count()` and `tally()`. You can look up their documentation to see how they work.)

```{r}
coffee_data %>% 
  group_by(Owner) %>% 
  summarize(total_farms = n())
```

The data that we are currently working with has one row for each combination of a **farm** and **country of origin**. We might want to know the total number of farms for each harvest year. To do this, we need to group by a variable, then use an aggregation function.

```{r}
coffee_data %>% 
  group_by(year) %>% 
  summarize(total_farms = n())
```

What was the total number of farms in South America per year? (Note: you'll need  the comparison operator `|`).

```{r}

```

## Data reshaping (`spread()` and `gather()`)

It can be helpful to think of tabular data as coming in two forms: wide data, and long data. Let's load two sets of sample data to get a sense of how these work.

```{r}
population <- read.csv("https://raw.githubusercontent.com/unolibraries/workshops/master/data-manipulation-r/population.csv")
population

cases <- read.csv("https://raw.githubusercontent.com/unolibraries/workshops/master/data-manipulation-r/cases.csv")
cases
```

The first thing we can notice about the first data frame is that it is very wide because it has a column for each of the years. The data is also suitable for reading because it reads like a table in a publication. We can read from left to right and see population or case values for each year. The difficulties of computing on or plotting the data will also become quickly apparent. How would you make a plot of the change over time? Or how would you filter by year, or summarize by year? For that matter, what do the numbers in the table represent, since they are not given an explicit variable name?

The problem with the table is that it is not *tidy data*, because the variables are not in columns and observations in rows. One of the variables is the year, but its values are in the column headers. And another of the variables is population, but its values are spread across rows and columns and it is not explicitly named. 

The `gather()` function from the [tidyr](https://cran.rstudio.com/web/packages/tidyr/) package lets us turn wide data into long data. We need to tell the function two kinds of information. First we need to tell it the name of the column to create from the column headers and the name of the implicit variable in the rows. In the example below, we create two new columns `year` and `population`. Then we also have to tell the function if there are any columns which should remain unchanged. If that were the case, we would remove it from the gathering using the same syntax as the `select()` function.

```{r}
population %>% 
  # we want the last two columns, so we tell R to select the range of columns from 2-3
  gather(year, population, 2:3) 
```

We can see the results above.

The inverse operation of `gather()` is `spread()`. With `spread()` we specify the name of the column which should become the new column headers (in this case `cases` and `population`), and then the name of the column to fill in underneath those new column headers (in this case, `key` and `value`). We can see the results below.

```{r}
cases %>% 
  spread(key, value)
```

We can also verify that programmatically using the `identical()` function.

Turning long data into wide is often useful when you want to create a tabular representation of data. (And once you have a data frame that can be a table, the `knitr::kable()` function is quite nice.) And some algorithms, such as clustering algorithms, expect wide data rather than tidy data.

For the exercise, we will use summary statistics of the number of white and black people in the Midwest by year.

```{r}
midwest_census <- read.csv("https://raw.githubusercontent.com/unolibraries/workshops/master/data-manipulation-r/midwest-census.csv")

states_by_year_and_race <- midwest_census %>% 
  group_by(year) %>% 
  summarize(white = sum(totalWhitePopulation, na.rm = TRUE),
            black = sum(totalAfAmPopulation, na.rm = TRUE),
            native_am = sum(totalIndianPopulation, na.rm = TRUE))
states_by_year_and_race
```

The data in `states_by_year_and_race` could be tidier still. While `white`, `black`, and `native_am` are variables, it is perhaps better to think of them as two different variables. One variable would be `race`, containing the racial descriptions that the Census used, and another would be `population`, containing the number of members. Using the `gather()` function, create that data frame.

```{r}

```

Now use that newly tidied data frame to create a wide data frame, where the years are the column headers and the racial descriptions are the rows.

```{r}

```

Now use the same tidied data to create a wide data frame where the racial descriptions are column headers and the years are rows.

```{r}

```

